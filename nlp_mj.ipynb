{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_mj",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy  #KoNLPy는한국어를 토큰화 할때 형태소 분석기를 사용하는데 \n",
        "                    #그때 필요한 라이브러리 pip로 설치를 진행해준다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE91Yrj1YjEl",
        "outputId": "80ec9865-69a1-465c-81bc-3f3ced87a4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Dpc5caW3am",
        "outputId": "004e8173-a50d-4f00-802e-358a193376c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련용 리뷰 개수 : 150000\n",
            "False\n",
            "149995\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#훈련데이터와 테스트 데이터를 다운로드 한다.\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "\n",
        "#Pandas를 이용하여 훈련데이터와 테스트데이터를 저장한다.\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')\n",
        "\n",
        "print('훈련용 리뷰 개수 :',len(train_data))\n",
        "train_data[:5]\n",
        "\n",
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
        "\n",
        "print(len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#현재 데이터는 영화 리뷰에 관한 데이터인데 알수없는 특수기호나 영어로 된 리뷰들은\n",
        "#분석에 필요가 없기 때문에 정규식을 사용하여 제거해준다.\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
        "train_data['document'].replace('', np.nan, inplace=True)# 공백은 Null 값으로 변경\n",
        "train_data = train_data.dropna(how = 'any') # Null 값 제거\n",
        "\n",
        "\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거"
      ],
      "metadata": {
        "id": "gEv8KHFke6b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 단어를 토큰화 하는 과정이 필요하다. NLP로 분석을 진행할 때 단어를 나눠서 단어의\n",
        "#의미와 문맥을 통해 분석하게 되는데 여기서 조사나 접속사 같은 불용어들은 필요가 없다\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "\n",
        "#토큰화를 위한 형태소 분석기는 KoNLPy의 Okt를 사용한다.\n",
        "okt = Okt()\n",
        "\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)\n",
        "\n",
        "\n",
        "print(X_train[:3])\n",
        "\n",
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y86WEiongbPE",
        "outputId": "e056d93c-cffa-4ad5-c887-b7467db3accc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 148740/148740 [10:55<00:00, 226.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['아', '더빙', '진짜', '짜증나다', '목소리'], ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49575/49575 [04:19<00:00, 190.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인공지능이 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을\n",
        "#진행해야 한다.\n",
        "#단어 집합이 생성되는 동시에 각 단어에 대한 고유한 정수가 부여된다.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "\n",
        "threshold = 3 # 등장 빈도수\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size) \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wwq8wfCkJ1w",
        "outputId": "8910223b-3988-4feb-d595-2bec53329c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 43752\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 24328\n",
            "단어 집합에서 희귀 단어의 비율: 55.604315231303715\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.8637396855052155\n",
            "단어 집합의 크기 : 19425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#빈 샘플 제거\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "\n",
        "#샘플들의 길이를 30으로 맞춰준다.\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n",
        "\n",
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKAz3x_Lmz-O",
        "outputId": "78905702-1e93-4191-c0b3-4022b657dd00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148509\n",
            "148509\n",
            "전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.44612784410373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM              #단어 임베딩, 시계열 데이터 처리를 위한 LSTM\n",
        "from tensorflow.keras.models import Sequential                          #MLP층을 쌓아가기 위함\n",
        "from tensorflow.keras.models import load_model                          #학습된 모델 저장\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint   #조기멈춤, 개선된 모델 저장\n",
        "\n",
        "#임베딩 벡터의 차원은 100\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "#단어 문맥의 파악이 필요하므로 LSTM을 사용하여 학습한다.\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#정확도가 개선되지 않을경우 조기 종료를 진행, 정확도가 개선되는 경우에만 모델을 저장\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "#모델 실행, 15번 반복\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
        "\n",
        "#저장된 모델의 정확도를 측정\n",
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1lceilxoZ_J",
        "outputId": "4dcb1189-6ad0-4644-e2ac-d798f920fce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.3860 - acc: 0.8250\n",
            "Epoch 00001: val_acc improved from -inf to 0.84762, saving model to best_model.h5\n",
            "1857/1857 [==============================] - 174s 91ms/step - loss: 0.3860 - acc: 0.8250 - val_loss: 0.3520 - val_acc: 0.8476\n",
            "Epoch 2/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.3251 - acc: 0.8590\n",
            "Epoch 00002: val_acc improved from 0.84762 to 0.85489, saving model to best_model.h5\n",
            "1857/1857 [==============================] - 168s 90ms/step - loss: 0.3251 - acc: 0.8590 - val_loss: 0.3417 - val_acc: 0.8549\n",
            "Epoch 3/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.3005 - acc: 0.8729\n",
            "Epoch 00003: val_acc improved from 0.85489 to 0.85913, saving model to best_model.h5\n",
            "1857/1857 [==============================] - 167s 90ms/step - loss: 0.3005 - acc: 0.8729 - val_loss: 0.3285 - val_acc: 0.8591\n",
            "Epoch 4/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.2810 - acc: 0.8833\n",
            "Epoch 00004: val_acc improved from 0.85913 to 0.86159, saving model to best_model.h5\n",
            "1857/1857 [==============================] - 167s 90ms/step - loss: 0.2810 - acc: 0.8833 - val_loss: 0.3257 - val_acc: 0.8616\n",
            "Epoch 5/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.2644 - acc: 0.8922\n",
            "Epoch 00005: val_acc did not improve from 0.86159\n",
            "1857/1857 [==============================] - 167s 90ms/step - loss: 0.2644 - acc: 0.8922 - val_loss: 0.3297 - val_acc: 0.8607\n",
            "Epoch 6/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.2496 - acc: 0.8996\n",
            "Epoch 00006: val_acc did not improve from 0.86159\n",
            "1857/1857 [==============================] - 169s 91ms/step - loss: 0.2496 - acc: 0.8996 - val_loss: 0.3346 - val_acc: 0.8583\n",
            "Epoch 7/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.2345 - acc: 0.9067\n",
            "Epoch 00007: val_acc did not improve from 0.86159\n",
            "1857/1857 [==============================] - 168s 91ms/step - loss: 0.2345 - acc: 0.9067 - val_loss: 0.3435 - val_acc: 0.8594\n",
            "Epoch 8/15\n",
            "1857/1857 [==============================] - ETA: 0s - loss: 0.2196 - acc: 0.9140\n",
            "Epoch 00008: val_acc did not improve from 0.86159\n",
            "1857/1857 [==============================] - 167s 90ms/step - loss: 0.2196 - acc: 0.9140 - val_loss: 0.3490 - val_acc: 0.8539\n",
            "Epoch 00008: early stopping\n",
            "1550/1550 [==============================] - 22s 14ms/step - loss: 0.3355 - acc: 0.8565\n",
            "\n",
            " 테스트 정확도: 0.8565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 결과를 가지고 리뷰를 예측해본다. 기존에 학습할때 가공했던것처럼 입력데이터를 가공 후 예측\n",
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
        "\n",
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')\n",
        "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnlAEs94oxoS",
        "outputId": "2f87a353-2efc-4c05-ffa4-942321845bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.24% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "97.71% 확률로 부정 리뷰입니다.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}